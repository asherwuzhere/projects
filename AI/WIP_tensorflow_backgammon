# This is still a work in progress.

import numpy as np
import random
import tensorflow as tf
from tensorflow import keras
from collections import deque
import copy

class Backgammon:
    def __init__(self):
        self.board = self.init_board()
        self.bar = {1: 0, -1: 0}
        self.borne_off = {1: 0, -1: 0}
        self.turn = 1  # 1: Player, -1: Opponent
        self.dice = []
        self.training_data = deque(maxlen=50000)  # Store training experiences
        self.epsilon = 1.0  # Exploration rate for RL
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.1
        self.gamma = 0.95  # Discount factor for future rewards

    def init_board(self):
        board = np.zeros(24, dtype=int)
        board[0], board[23] = 2, -2
        board[5], board[18] = -5, 5
        board[7], board[16] = -3, 3
        board[11], board[12] = 5, -5
        return board

    def receive_dice_input(self):
        self.dice = [random.randint(1, 6), random.randint(1, 6)]
        if self.dice[0] == self.dice[1]:  # Doubles
            self.dice *= 2

    def generate_legal_moves(self):
        moves = []
        for die in self.dice:
            for point in range(24):
                if self.board[point] * self.turn > 0:
                    target = point + die * self.turn
                    if 0 <= target < 24 and (self.board[target] * self.turn >= -1):
                        moves.append((point, target))
        return moves

    def apply_move(self, move):
        start, end = move
        piece = np.sign(self.board[start])
        self.board[start] -= piece
        if self.board[end] == -piece:
            self.board[end] = 0
            self.bar[-piece] += 1
        self.board[end] += piece

    def evaluate_board(self):
        return sum(self.board)  # Simple evaluation function

    def best_move(self, model):
        legal_moves = self.generate_legal_moves()
        if not legal_moves:
            return None

        # Use Monte Carlo Tree Search (MCTS) to simulate moves
        move = self.monte_carlo_tree_search(legal_moves, simulations=100)
        return move

    def monte_carlo_tree_search(self, legal_moves, simulations=100):
        move_scores = {move: 0 for move in legal_moves}

        for _ in range(simulations):
            for move in legal_moves:
                temp_game = copy.deepcopy(self)  # Copy game state
                temp_game.apply_move(move)
                score = temp_game.simulate_random_game()
                move_scores[move] += score

        return max(move_scores, key=move_scores.get)  # Best move

    def simulate_random_game(self):
        temp_game = copy.deepcopy(self)
        while max(temp_game.borne_off.values()) < 15:
            legal_moves = temp_game.generate_legal_moves()
            if legal_moves:
                move = random.choice(legal_moves)
                temp_game.apply_move(move)
            temp_game.turn *= -1  # Switch turns
        return temp_game.evaluate_board()

    def play_turn(self, model):
        self.receive_dice_input()
        move = self.best_move(model)
        if move:
            self.apply_move(move)

        # Store training data
        next_state = np.concatenate([self.board, [self.turn]]).reshape(1, -1)
        reward = self.evaluate_board()
        self.training_data.append((self.board, move, reward, next_state))

        self.turn *= -1  # Switch turn

    def train_model(self, model, batch_size=32):
        if len(self.training_data) < batch_size:
            return  # Not enough data to train

        batch = random.sample(self.training_data, batch_size)
        states, targets = [], []

        for state, move, reward, next_state in batch:
            q_values = model.predict(state.reshape(1, -1))[0]
            future_q_values = max(model.predict(next_state.reshape(1, -1))[0])
            q_values[move] = reward + self.gamma * future_q_values
            states.append(state)
            targets.append(q_values)

        model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Define the neural network model
def create_model():
    inputs = keras.Input(shape=(25,))
    x = keras.layers.Dense(64, activation='relu')(inputs)
    x = keras.layers.Dense(128, activation='relu')(x)
    outputs = keras.layers.Dense(24, activation='linear')(x)  # Output Q-values

    model = keras.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='mse')
    return model

# Train the bot using self-play
def train_bot():
    model = create_model()
    game = Backgammon()

    for episode in range(1000):  # Train for 1000 games
        game.play_turn(model)
        game.train_model(model)

        if episode % 100 == 0:
            print(f"Episode {episode}, Epsilon: {game.epsilon}")

    return model

# Train the model
model = train_bot()

# Save trained model
model.save("backgammon_bot_mcts.h5")
